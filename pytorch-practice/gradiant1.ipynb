{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1.0, 2.0 , 3.0]\n",
    "y_data = [2.0 , 4.0, 6.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return x * w \n",
    "\n",
    "def loss (x ,y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) *(y_pred - y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient is d_loss/d_w\n",
    "\n",
    "our loss function is (x*w - y)^2\n",
    "d_loss/d_w is then 2*x*(x*w - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y):\n",
    "    return 2*x * (x*w - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w=  1.0\n",
      "\tgrad:  1.0 2.0 -2.0\n",
      "\tgrad:  2.0 4.0 -7.84\n",
      "\tgrad:  3.0 6.0 -16.23\n",
      "progress: 0 w= 1.26 loss= 4.92\n",
      "w=  1.260688\n",
      "\tgrad:  1.0 2.0 -1.48\n",
      "\tgrad:  2.0 4.0 -5.8\n",
      "\tgrad:  3.0 6.0 -12.0\n",
      "progress: 1 w= 1.45 loss= 2.69\n",
      "w=  1.453417766656\n",
      "\tgrad:  1.0 2.0 -1.09\n",
      "\tgrad:  2.0 4.0 -4.29\n",
      "\tgrad:  3.0 6.0 -8.87\n",
      "progress: 2 w= 1.6 loss= 1.47\n",
      "w=  1.5959051959019805\n",
      "\tgrad:  1.0 2.0 -0.81\n",
      "\tgrad:  2.0 4.0 -3.17\n",
      "\tgrad:  3.0 6.0 -6.56\n",
      "progress: 3 w= 1.7 loss= 0.8\n",
      "w=  1.701247862192685\n",
      "\tgrad:  1.0 2.0 -0.6\n",
      "\tgrad:  2.0 4.0 -2.34\n",
      "\tgrad:  3.0 6.0 -4.85\n",
      "progress: 4 w= 1.78 loss= 0.44\n",
      "w=  1.7791289594933983\n",
      "\tgrad:  1.0 2.0 -0.44\n",
      "\tgrad:  2.0 4.0 -1.73\n",
      "\tgrad:  3.0 6.0 -3.58\n",
      "progress: 5 w= 1.84 loss= 0.24\n",
      "w=  1.836707389300983\n",
      "\tgrad:  1.0 2.0 -0.33\n",
      "\tgrad:  2.0 4.0 -1.28\n",
      "\tgrad:  3.0 6.0 -2.65\n",
      "progress: 6 w= 1.88 loss= 0.13\n",
      "w=  1.8792758133988885\n",
      "\tgrad:  1.0 2.0 -0.24\n",
      "\tgrad:  2.0 4.0 -0.95\n",
      "\tgrad:  3.0 6.0 -1.96\n",
      "progress: 7 w= 1.91 loss= 0.07\n",
      "w=  1.910747160155559\n",
      "\tgrad:  1.0 2.0 -0.18\n",
      "\tgrad:  2.0 4.0 -0.7\n",
      "\tgrad:  3.0 6.0 -1.45\n",
      "progress: 8 w= 1.93 loss= 0.04\n",
      "w=  1.9340143044689266\n",
      "\tgrad:  1.0 2.0 -0.13\n",
      "\tgrad:  2.0 4.0 -0.52\n",
      "\tgrad:  3.0 6.0 -1.07\n",
      "progress: 9 w= 1.95 loss= 0.02\n",
      "loss= 0.02\n"
     ]
    }
   ],
   "source": [
    "w=1.0\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(\"w= \", w)\n",
    "    l_sum = 0 \n",
    "    for x , y in zip(x_data , y_data):\n",
    "        grad = gradient(x,y)\n",
    "        w = w-0.01* grad\n",
    "        y_pred = forward(x)\n",
    "        l = loss(x, y)\n",
    "        l_sum += l\n",
    "        print(\"\\tgrad: \", x, y, round(grad, 2))\n",
    "\n",
    "    print(\"progress:\", epoch, \"w=\", round(w, 2), \"loss=\", round(l, 2))\n",
    "\n",
    "print(\"loss=\", round(l, 2))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
